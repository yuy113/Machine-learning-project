 % I USED PDFLATEX TO COMPILE THIS PRESENTATION BECAUSE OF THE
%   GRAPHIC FILE TYPES I HAD.
% IF YOU WOULD LIKE TO COMPILE IT YOURSELF, COMMENT OUT
%   ALL OF THE FIGURES AND MOVIE AND YOU WILL BE ABLE TO
%   COMPILE WITH BOTH PDFLATEX AND LATEX+DVIPS.

\documentclass[10pt]{beamer}

% USE THE "HANDOUT" OPTION TO CREATE HANDOUTS
%\documentclass[handout]{beamer}

% THIS PUTS TWO SLIDES ON EACH PAGE
%\usepackage{pgfpages}
%\pgfpagesuselayout{2 on 1}[a4paper,border shrink=5mm]

\usetheme{Boadilla}
\usepackage{time}             % date and time
\usepackage{graphicx, epsfig}
\usepackage[T1]{fontenc}      % european characters
\usepackage{amssymb,amsmath}  % use mathematical symbols
\usepackage{palatino}         % use palatino as the default font
\usepackage{multimedia}
\usepackage{subfigure}
\usepackage{mathrsfs}
\usepackage{latexsym}
\usepackage{amsmath,epsfig,epsf,psfrag}
\usepackage{amssymb}
\usepackage{biblatex}
%\usepackage{natbib}
%\usepackage{bibentry}

\usepackage{multicol}
\usepackage{multirow}
\usepackage[absolute,overlay]{textpos}
\usepackage{graphicx}
\usecolortheme{crane}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bi}{\begin{itemize}}
	\newcommand{\ei}{\end{itemize}}
\newcommand{\bn}{\begin{enumerate}}
	\newcommand{\en}{\end{enumerate}}
\newcommand{\bs}{\boldsymbol}
\newcommand{\bey}{\begin{eqnarray}}
\newcommand{\eey}{\end{eqnarray}}
\newcommand{\beyn}{\begin{eqnarray*}}
	\newcommand{\eeyn}{\end{eqnarray*}}
\newcommand{\ba}{\begin{array}}
	\newcommand{\ea}{\end{array}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Sum}{\displaystyle\sum}
\newcommand{\Prod}{\displaystyle\prod}
% CREATES SHADED INSTEAD OF HIDDEN OVERLAYS
%\setbeamercovered{transparent}
% SOME COMMANDS I'VE CREATED FOR LONG COMMANDS I USE OFTEN
\newcommand{\ds}{\displaystyle}
\newcommand{\ve}{\varepsilon}
\newcommand{\der}[2]{\frac{d #1}{d #2}}
\newcommand{\pder}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\inv}{^{-1}}
\newcommand{\mat}[1]{\textbf{#1}}
\title{Particle EM in Latent Dirichlet Allocation model}
\author{Yubing Yao}
 % COMMAND UNIQUE TO BEAMER
\date{\today}
\begin{document}
	\maketitle{}
	\vfill
%	\tableofcontents
%	\vfill
	\section{Introduction}

\begin{frame}
	\frametitle{Introduction-Particle EM}
	\begin{enumerate}
		\item The Particle EM Algorithm was introduced by Veronika Rockova in Bayesian variable selection to find the best multiple point approximation of posterior marginal distribution in 2016. 
	\vspace{2ex}
		\item The particle EM algorithm is a population based optimization method, and aims to overcome the vulnerability of local entrapment of the traditional EM algorithm when dealing with the multi-modal posterior/likelihood.
			\vspace{2ex}
		\item The particle EM algorithm explores the whole search space by multiple repulsive particles and tries to capture the multiple modes of posterior distribution. Thus it can increase the possibility to identify a global mode by discovering a more comprehensive set of posterior modes.
		
		
	\end{enumerate}
\end{frame}
\begin{frame}
	\frametitle{A simple illustrative example comparing EM and particle EM}
	Two collinear predictors-$\bs{\beta}=(\beta_1,\beta_2)$, The posterior distribution-$\pi(\bs{\beta}|\bs{Y})$ have 4 modes with mode 3-the global mode.
	\begin{figure}[b]
		\includegraphics[width = 12cm,height=6.5cm]{pemex1.png}
		\centering
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Introduction- Latent Dirichlet Allocation(LDA) model }
The Latent Dirichlet Allocation(LDA) model is a probabilistic model for collections of discrete data such as text corpora introduced by D. Blei et al. 2003 with model parameters-$\bs{\alpha},\bs{\beta}$. 
\begin{columns}[T] % align columns\begin{column}{.48\textwidth}
	\begin{column}{.48\textwidth}
		\vspace{5ex}
\begin{equation*}
	\ba{rcl}
		p(\mathcal{D}|\bs{\alpha},\bs{\beta})&=&\Prod_{d=1}^{D}\int p(\theta_d,\bs{z}_d,\bs{w}_d|\bs{\alpha},\bs{\beta})d\theta_d\\
		&=&\Prod_{d=1}^{D}\int p(\theta_d|\bs{\alpha})\Prod_{n_d=1}^{N}p(\bs{z}_{dn_d}|\theta_d)\\
		&&\times p(\bs{w}_{dn_d}|\bs{z}_{dn_d},\bs{\beta})d\theta_d
	\ea
\end{equation*}
				\end{column}%\hfill%
					\begin{column}{.48\textwidth}
	\begin{figure}[b]
		\includegraphics[width = 4cm,height=5cm]{LDAgraph.png}
	\end{figure}
\end{column}%
\end{columns}	 
\end{frame}		

\begin{frame}
	\frametitle{Variational Inference in LDA model }
	Two free variational parameters-$\bs{\gamma},\bs{\phi}$ was introduced in D. Blei et al. 2003 to break the coupling between the parameters-$\bs{\theta}$ and $\bs{\beta}$ in LDA such that:\\
	\[
	q(\theta_d,\bs{z}_d|\bs{\gamma},\bs{\phi})=q(\theta_d|\bs{\gamma})\Prod_{n=1}^{N_d}q(\bs{z}_{nd}|\bs{\phi}_{dn})
	\]	
\begin{columns}[T] % align columns\begin{column}{.48\textwidth}
\begin{column}{.61\textwidth}
			ELBO of variational inference in LDA model can be represented as:
\begin{equation*}
\ba{cl}
\Sum_{d=1}^{D}\left(\E_{q}[\log p(\theta_d|\alpha)]\right.&\\
+\E_{q}[\log p(\bs{z}_d|\theta_d)]&\\
&\\
+\E_{q}[\log p(\bs{w}_d|\bs{z}_d,\beta)]&\\
&\\
-\E_{q}[\log q(\theta_d|\bs{\gamma}_d)]&\\
&\\
-\left.\E_{q}[\log q(\bs{z}_{n_d}|\bs{\phi}_{dn})]\right)
\ea
\end{equation*}			
\end{column}%\hfill%
		
\begin{column}{.39\textwidth}
	\begin{figure}[b]
		\includegraphics[width = 5cm,height=5cm]{ldavi.png}
		\centering
	\end{figure}
\end{column}%
\end{columns}	
\end{frame}		

\section{Method}	
	\begingroup
	\small% \small i		
\begin{frame}
	\frametitle{Particle approximation and Evidence Lower Bound}
	\bi
\item We use a weighted mixture of atoms to approximate $\pi(\bs{z}_{dn}|\mathcal{D}), n=1,2,\ldots,N_d; d=1,2,\ldots,D$, and we denote:\\
\[
q_{PEM}(\bs{z}_{dn}|\bs{\Gamma}_{dn},\bs{\omega})=\Sum_{p=1}^{P}\omega_{p}\mathbb{I}\{\bs{z}_{dn}=\bs{z}_{pdn}\}
\]
where $\bs{\Gamma}_{dn}=[\bs{z}_{1dn},\bs{z}_{2dn},\ldots,\bs{z}_{Pdn}]$,corresponding importance weights-$\bs{\omega}=(\omega_{1},\omega_{2},\ldots,\omega_{P})^T$, where $\sum_{p=1}^{P}\omega_{p}=1,\forall d=1,\ldots,D,n=1,2,\ldots,N_d;0\le\omega_{p}\le 1,\forall p=1,2,\ldots,P$.\\
\vspace{1.5ex}
\item The corresponding Evidence Lower Bound (ELBO)-$\mathcal{L}_{\lambda}(\bs{\gamma},\bs{\Gamma},\bs{\omega};\bs{\theta},\alpha,\beta)$ is:
\be\label{elbo3}
\ba{rl}
\Sum_{d=1}^{D}\left(\E_{q}[\log p(\theta_d|\alpha)]+\E_{q_{PEM}}[\log p(\bs{z}_d|\theta_d)]+\E_{q_{PEM}}[\log p(\bs{w}_d|\bs{z}_d,\beta)]\right.&\\
\left.-\E_{q}[\log q(\theta_d|\bs{\gamma}_d)]-\lambda\E_{q_{PEM}}[\log q_{PEM}(\bs{z}_{n_d}|\bs{\Gamma}_d,\bs{\omega}_d)]\right)&\\
\ea
\ee
where $
\lambda\ge 0$.\\
\ei
\end{frame}	
\endgroup	

\begin{frame}
	\frametitle{Particle EM-E step with single particle}
	\bi
	\item With single particle $P=1$ and fix some document $d,\bs{w}_d$  then $\omega_1=1$, very similar to optimal estimator of variational parameters-$\bs{\phi}_{dn},\bs{\gamma}_d$ in LDA model.
	\vspace{1ex}
\item The value of estimated $\hat{\phi}_{dni}$ maximizing the ELBO is:
\[
\hat{\phi}_{dni}=\frac{\beta_{iv_d}^{1/\lambda}\exp((\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))/\lambda)}{\sum_{i'=1}^{k}\beta_{i'v_d}^{1/\lambda}\exp((\Psi(\gamma_{di'})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))/\lambda)}
\]
	\vspace{1ex}
\item The value of estimated $\hat{\gamma}_{di}$ maximizing the ELBO is:\\
\[
\hat{\gamma}_{di}=\alpha_i+\sum_{n=1}^{N_d}\phi_{dni}
\]
	\ei
	\end{frame}	
\begin{frame}
	\frametitle{Particle EM-M step with single particle}
	\bi
	\item In M step with single particle-$P=1$, with all the documents-
	$\bs{w}_d,d=1,\ldots,D$, maximize the  ELBO with respect to model parameters-$\bs{\alpha,}\bs{\beta}$.
	\vspace{1ex}
	\item The value of estimated $\hat{\beta}_{ij}$ maximizing the ELBO is:
\[
\hat{\beta}_{ij}=\frac{\sum_{d=1}^{D}\sum_{n=1}^{N_d}\phi_{dni}w_{dn}^j}{\sum_{d=1}^{D}\sum_{n=1}^{N_d}\sum_{j'=1}^{V}\phi_{dni}w_{dn}^{j'}}
\]
	\vspace{1ex}
	\item The estimation of $\alpha_{i},i=1,2,\ldots,k$ by maximizing the ELBO can be solved numerically by Newton-Raphson method.\\
	\ei
\end{frame}

\begin{frame}
	\frametitle{Particle EM-E step with multiple particles}
	\bi
	\item With $P>1$, we need alternatively update the particle location-$[\bs{\phi}_{1dn},\bs{\phi}_{2dn},\ldots,\bs{\phi}_{Pdn}]$ and their corresponding importance weights-$(\bs{\omega}_{1},\bs{\omega}_{2},\ldots,\bs{\omega}_{P})^T,\forall n=1,2,\ldots,N_d; d=1,\ldots,D$.
	\item Apply the Newton-Raphson method, for each fixed $n=1,2,\ldots,N_d, d=1,2,\ldots,D, i=1,2,\ldots,k$, denote $\bs{\phi}_{dni}=[\phi_{1dni},\phi_{2dni},\ldots,\phi_{Pdni}]^T$, then by iterating the equation below we can find the maximal-$\bs{\phi}_{dni}$:\\
	\[
	\bs{\phi}_{dni(new)}=\bs{\phi}_{dni(old)}-H(\bs{\phi}_{dni(old)})^{-1}g(\bs{\phi}_{dni(old)})
	\]
	\item The value of estimated importance weight-$\hat{\omega}_p$ is:
	\be\label{omg3}
	\ba{rcl}
	\hat{\omega}_p
	&=&\frac{\sum_{d=1}^{D}\sum_{n=1}^{N_d}\sum_{i=1}^{k}\left(\phi_{pdni}(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))+\phi_{pdni}\log\beta_{iv_d}\right)}{\sum_{p'=1}^{P}\sum_{d=1}^{D}\sum_{n=1}^{N_d}\sum_{i=1}^{k}\left(\phi_{p'dni}(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))+\phi_{p'dni}\log\beta_{iv_d}\right)}
	\ea
	\ee
	\item The value of estimated $\hat{\gamma}_{di}$ maximizing the ELBO is:\\
	\[
\hat{\gamma}_{di}=\alpha_i+\sum_{n=1}^{N_d}\sum_{p=1}^{P}\omega_{p}\phi_{pdni}
	\]
	\ei
\end{frame}	


	\begingroup
	\small% \small i
\begin{frame}
	\frametitle{Particle EM-M step with multiple particles}
	\bi
	\item In M step with multiple particles-$P>1$, with all the documents- $\bs{w}_d,d=1,\ldots,D$, maximize the ELBO with respect to model parameters-$\bs{\alpha,}\bs{\beta}$.
	\item The value of estimated $\hat{\beta}_{ij}$ maximizing the ELBO is:
	\[
	\hat{\beta}_{ij}=\frac{\sum_{d=1}^{D}\sum_{n=1}^{N_d}\sum_{p=1}^{P}\omega_{p}\phi_{pdni}w_{dn}^j}{\sum_{d=1}^{D}\sum_{n=1}^{N_d}\sum_{j'=1}^{V}\sum_{p=1}^{P}\omega_{p}\phi_{pdni}w_{dn}^{j'}}
	\]
		\item The estimation of $\alpha_{i},i=1,2,\ldots,k$ by maximizing the ELBO can be solved numerically by Newton-Raphson method with a Hessian matrix with special structure:
		\[
		\bs{\alpha}_{(new)}=\bs{\alpha}_{(old)}-H'(\bs{\alpha}_{(old)})^{-1}g'(\bs{\alpha}_{(old)})
		\]
		where $H'(\bs{\alpha}),g'(\bs{\alpha})$ are the Hessian matrix and gradient respectively at the point-$\bs{\alpha}$ defined above.\\
		And the Hessian matrix $H'(\bs{\alpha})$ with the special form:\\
		\[
		H'(\bs{\alpha})=diag(\bs{h})-\Psi'(\sum_{j=1}^{k}\alpha_j)\bs{1}\bs{1}^T
		\]
		where $\bs{h}=[D\Psi'(\alpha_1),D\Psi'(\alpha_2),\ldots,D\Psi'(\alpha_k)]^T$\\
	\ei
\end{frame}	
\endgroup	

\begin{frame}
	\frametitle{Stochastic Variational Inference in LDA}
	Stochastic variational inference is stochastic optimization with \textbf{noisy natural gradients} to optimize the variational objective function. \\
	Which are local and global in LDA?
	\begin{itemize}
		\item \textbf{Local:} Document $d$, word $w_{d,1:N}$, topic parameters $\theta_d$, topic assignment $z_{d,1:N}$.
		\item \textbf{Global:} hidden variable $\beta_{1:K}$, govern parameter $\lambda_{k,1:V}$
	\end{itemize}
	$\Rightarrow$ The $V -$dimensional variational distribution for $\beta_k$:
	$$q(\beta_k)=\text{Dirichlet}(\lambda_k)$$
\end{frame}

\begin{frame}
	\frametitle{SVI Algorithm in LDA}
	\begin{table}
		\resizebox{\textheight}{!}{%
			\begin{tabular}{|p{12cm}|}
				\hline
				\begin{enumerate}
					\item Initialize $\lambda^{(0)}$ randomly.
					\item Set the step-size schedule $\rho_t$ appropriately
					\item \textbf{repeat}
					\item \hspace{1ex} Sample a document $w_d$ uniformly from the data set.
					\item \hspace{1ex} Initialize $\gamma_{dk}=1$, for $k\in\{1,\hdots,K\}$.
					\item \hspace{1ex} \textbf{repeat}
					\item \hspace{5ex} For $n \in \{1,\hdots, N\}$ set $$\phi_{dn}^k \propto \exp\{\mathbb{E}[\log\theta_{dk}] + \mathbb[\log \beta_{k,w_{dn}}]\}, k\in \{1,\hdots,K\}.$$
					\item \hspace{5ex} Set $\gamma_d=\alpha+\sum_{n}\phi_{dn}$.
					\item \hspace{3ex} \textbf{until} local parameters $\phi_{dn}$ and $\gamma_d$ converge.
					\item \hspace{3ex} $k\in\{1,\hdots,K\}$ set intermediate topics $$\hat{\lambda}_k=\eta+D\sum_{n=1}^N \phi_{dn}^k w_{dn}.$$
					\item \hspace{3ex} $\lambda^{(t)}=(1-\rho_t)\lambda^{(t-1)}+\rho_t \hat{\lambda}$
					\item \textbf{until} forever
				\end{enumerate}\\
				\hline
			\end{tabular}
		}	
	\end{table} 	  	
\end{frame}



\end{document}