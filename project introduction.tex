\documentclass[a4paper]{article}
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{chains,positioning}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{latexsym}
\usepackage{amsmath,epsfig,epsf,psfrag,centernot}
\usepackage{amssymb}
\usepackage[authoryear]{natbib}
\usepackage{amssymb, latexsym}
\usepackage[normalem]{ulem}
\usepackage{amsmath} 
\usepackage{amsthm}
\usepackage{bm}
\usepackage[mathscr]{eucal}
\usepackage{enumerate}
\usepackage{lineno}
\usepackage{pdflscape}
\usepackage{float}
\restylefloat{table}
\usepackage{graphicx}
\usepackage{caption,booktabs,array}
\usepackage{geometry}
\usepackage[T1]{fontenc}
\usepackage{multicol}
\usepackage{multirow}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bi}{\begin{itemize}}
	\newcommand{\ei}{\end{itemize}}
\newcommand{\bs}{\boldsymbol}
\newcommand{\bey}{\begin{eqnarray}}
\newcommand{\eey}{\end{eqnarray}}
\newcommand{\beyn}{\begin{eqnarray*}}
	\newcommand{\eeyn}{\end{eqnarray*}}
\newcommand{\ba}{\begin{array}}
	\newcommand{\ea}{\end{array}}
\DeclareMathOperator*{\argmin}{\arg\!\min}

\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Sum}{\displaystyle\sum}
\newcommand{\Prod}{\displaystyle\prod}
\newcommand{\bigCI}{\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10mu\perp$}}}}
\newcommand{\nbigCI}{\centernot{\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10mu\perp$}}}}}

\renewcommand*{\maketitle}{%
	\noindent
	\begin{center}
		\framebox{
			\vbox{
				\hbox to 5.78in { \textbf{ UMass Stat697ML: Statistical Machine Learning} \hfill \@\today}
				\vspace{4mm}
				\hbox to 5.78in { {\Large \hfill \@Final project \hfill } }
				\vspace{2mm}
				\hbox to 5.78in { \textit{ \@Yubing Yao \hfill } }
			}
		}	
	\end{center}
	\vspace*{4mm}
}


%
% Useful symbols
%
\newcommand{\ihat}{\hat{\imath}}
\newcommand{\jhat}{\hat{\jmath}}
\newcommand{\Nat}{\bf N}                               % natural numbers
\newcommand{\Int}{\mathbf{Z}}                          % integers
\newcommand{\Bool}{\it Bool}                           % booleans
\newcommand{\true}{\tt t}
\newcommand{\false}{\tt f}
\newcommand{\I}{\cal I}                                % interpretations
\newcommand{\M}{\cal M}                                % meaning functions
\newcommand{\A}{\cal A}                                % arithmetic interpretation
\newcommand{\R}{\mathbb{R}}
\newcommand{\B}{\cal B}                                % binary word interpretation
\newcommand{\TIME}{\mathop{\rm TIME}\nolimits}
\newcommand{\NTIME}{\mathop{\rm NTIME}\nolimits}
\newcommand{\SPACE}{\mathop{\rm SPACE}\nolimits}
\newcommand{\NSPACE}{\mathop{\rm NSPACE}\nolimits}
\newcommand{\union}{\cup}
\newcommand{\intersect}{\cap}
%\newcommand{\implies}{\Rightarrow}

%
% Probability symbols
%
\newcommand{\Pm}{\mathbb{P}}
\newcommand{\F}{\mathcal{F}}

% 
% Useful functions
%
\newcommand{\abs}[1]{\mathify{\left| #1 \right|}}
\renewcommand{\Pr}[1]{\mathify{\mbox{Pr}\left(#1\right)}}
%\newcommand{\E}[1]{\mathify{\mbox{E}\left[#1\right]}}
\newcommand{\Exp}[1]{\mathify{\mbox{Exp}\left[#1\right]}}
\newcommand{\Tr}[1]{\mathify{\mbox{Tr}\left(#1\right)}}
\newcommand{\set}[1]{\mathify{\left\{ #1 \right\}}}
\newcommand{\cset}[2]{\set{#1\ :\ #2}}  % a conditional notation to define sets
\newcommand{\lset}[2]{\set{#1,\ldots,#2}} % set {from,...,to}
\newcommand{\suchthat}{\vert}
\newcommand{\st}{\suchthat}
\newcommand{\ind}{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

%
% For pseudo-code
%
\newcommand{\FOR}{{\bf for}}
\newcommand{\TO}{{\bf to}}
\newcommand{\DO}{{\bf do}}
\newcommand{\WHILE}{{\bf while}}
\newcommand{\AND}{{\bf and}}
\newcommand{\IF}{{\bf if}}
\newcommand{\THEN}{{\bf then}}
\newcommand{\ELSE}{{\bf else}}



%
% Useful environments-- theorem-like
%
%\newtheorem{observation}[theorem]{Observation}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{claim}[theorem]{Claim}
%\newtheorem{assumption}[theorem]{Assumption}

%
% Useful environments for proofs
%
%\newenvironment{proof}{\noindent{\bf Proof:}\hspace*{1em}}{\qed\bigskip}
\newenvironment{proof-sketch}{\noindent{\bf Sketch of Proof:}\hspace*{1em}}{\qed\bigskip}
\newenvironment{proof-idea}{\noindent{\bf Proof Idea:} \hspace*{1em}}{\qed\bigskip}
\newenvironment{proof-of-lemma}[1]{\noindent{\bf Proof of Lemma #1:}\hspace*{1em}}{\qed\bigskip}
\newenvironment{proof-attempt}{\noindent{\bf Proof Attempt:}\hspace*{1em}}{\qed\bigskip}
\newenvironment{proofof}[1]{\noindent{\bf Proof}
	of #1:\hspace*{1em}}{\qed\bigskip}
\newenvironment{remark}{\noindent{\bf Remark:}\hspace*{1em}}{\bigskip}


\newcommand{\eqdef}{\stackrel{\rm def}{=}}       % ``equals by definition''
\newcommand{\hint}{{\em Hint}:\ }              % for in-line hints
%\newcommand{\note}{{\em Note}:\ }              % for in-line notes
%\newcommand{\remark}{{\em Remark}\/:\ }              % for in-line remarks  
\newcommand{\sem}[1]{[\![\,#1\,]\!]}
\begin{document}
\maketitle
\section{Project Statement}
We plan to introduce and develop the particle EM algorithm (Rockova V. 2016) under the framework of Latent Dirichlet Allocation (LDA) model (D. Blei et al. 2003). We will derive the adapted Evidence Lower Bound (ELBO) in Variational Bayesian inference based on a simpler distribution with particle EM approximation. In our project, we are planning to identify the multi-modal posterior in LDA model and to implement particle EM algorithm motivated by population-based optimization. 
\\
The Particle EM Algorithm was first introduced by Veronika Rockova in Bayesian variable selection to find the best multiple point approximation of posterior marginal distribution in 2016. The particle EM algorithm explores the whole search space by multiple repulsive particles and try to capture the multiple modes of posterior distribution. A lot of other variation inference approaches have already been implemented to LDA such as stochastic variational inference (Matthew D. Hoffman, et al. 2013) and nonparametric variational inference approach (Samuel J. Gershman, et al. 2012). So we are planning to compare those three approaches (Particle EM Algorithm, Stochastic variational inference and nonparametric variational inference) within LDA model.
\section{Latent Dirichlet Allocation(LDA) model framework}
The Latent Dirichlet Allocation(LDA) model is a probabilistic model for collections of discrete data such as text corpora introduced by (D. Blei et al. 2003). Following the same notation as D. Blei et al. 2003:\\
\bi
\item Denote a collection of $M$ documents-$\mathcal{D}=\{\bs{w}_1,\bs{w}_2,\ldots,\bs{w}_M\}$.
\item A document has a sequence of $N$ words denoting $\bs{w}=\{w_1,w_2,\ldots,w_N\}$.
\item A word defined as an item from a vocabulary indexed by $\{1,2,\ldots,V\}$. Represent words using unit-basis vectors that have a single component equal to
one and all other components equal to zero. Thus, using superscripts to denote components, the $v$th word in the vocabulary is represented by a $V$-vector $w$ such that $w^v = 1$ and $w^u=1$ for
$u\neq v$.
\ei
Assume the documents are represented as random mixture over latent topics in LDA model, without slight modification, firstly we fix the number of latent topics-$N$ is fixed,then the following generative process for each document-$\bs{w}$ in a corpus-$\mathcal{D}$:\\
\bi
\item[1.] Choose $\theta\sim Dir(\alpha)$
\item[2.] For each of the $N$ words-$w_n$:
\subitem(a) Choose a topic $z_n\sim Multinomial(\theta)$
\subitem(b) Choose a word $w_n$ from $p(w_n|z_n,\beta)$
\ei
Denote $k\times V$ matrix $\beta$ where $\beta_{ij}=p(w^j=1|z^i=1),j=1,\ldots,V,i=1,2,\ldots,k$, and $w_{n_d}^j=1$ of $j$th component of the word $w_{n_d},j=1,\ldots,V,n_d=1,\ldots,N_d,d=1,2,\ldots,D$.
\\
Given the parameter-$\alpha$ and $\beta$, for a corpus-$\mathcal{D}$ with  $d=1,\ldots,D$ documents,
then joint distribution of a topic mixture-$\theta_d$, a set of $N_d$ topics-$\bs{z}_d$ and a set of $N_d$ words-$\bs{w}_d$ is given by:
\[
p(\mathcal{D}|\alpha,\beta)=\Prod_{d=1}^{D}\int p(\theta_d,\bs{z}_d,\bs{w}_d|\alpha,\beta)d\theta_d=\Prod_{d=1}^{D}\int p(\theta_d|\alpha)\Prod_{n_d=1}^{N}p(\bs{z}_{dn_d}|\theta_d)p(\bs{w}_{dn_d}|\bs{z}_{dn_d},\beta)d\theta_d
\]
where $p(\theta_d|\alpha)$ assuming $K$ dimensional vector of $\theta_d$:\\
\[
p(\theta_d|\alpha)=\frac{\Gamma(\sum_{i=1}^{k}\alpha_i)}{\prod_{i=1}^{k}\Gamma(\alpha_i)}\theta_1^{\alpha_1-1}\cdots\theta_k^{\alpha_k-1}
\]
And denote $\phi_{n_di}=I(z_{w_{n_d}}^i=1)$, that is, $n_d$th word is generated from latent topic-$i$, then $p(\bs{w}_{dn}|\bs{z}_{dn},\beta)$ can be represented as:\\
\[
p(\bs{w}_{dn}|\bs{z}_{dn},\beta)=\Prod_{i=1}^{k}\Prod_{j=1}^{V}(\theta_{i}\beta_{ij})^{w_{n_d}^{j}}=\Prod_{i=1}^{k}\Prod_{j=1}^{V}(\beta_{ij})^{w_{n_d}^{j}\phi_{n_di}}
\]
\section{Variational Inference in Latent Dirichlet Allocation(LDA) model}
Two free variational parameters-$\bs{\gamma},\bs{\phi}$ was introduced in D. Blei et al. 2003 to break the coupling between  model parameters-$\theta$ and $\beta$ in LDA such that:\\
\[
q(\theta_d,\bs{z}_d|\gamma,\phi)=q(\theta|\gamma)\Prod_{n_d=1}^{N_d}q(\bs{z}_{n_d}|\phi_{n_d})
\]
For any fixed $d$, we can achieve a evidence lower bound using the variational distribution-$q(\theta_d,\bs{z}_d|\gamma,\phi)$,\\
\be\label{elbo1}
\ba{rcl}
\log p(\bs{w}_d|\alpha,\beta)&=&\log\int\Sum_{
\bs{z}_d}p(\theta_d,\bs{z}_d,\bs{w}_d|\alpha,\beta)d\theta_d\\
&=&\log\int\Sum_{
	\bs{z}_d}\frac{p(\theta_d,\bs{z}_d,\bs{w}_d|\alpha,\beta)q(\theta_d,\bs{z}_d|\gamma,\phi)}{q(\theta_d,\bs{z}_d|\gamma,\phi)}d\theta_d\\
&\ge&\int\Sum_{
	\bs{z}_d}q(\theta_d,\bs{z}_d|\gamma,\phi)\log p(\theta_d,\bs{z}_d,\bs{w}_d|\alpha,\beta)-\int\Sum_{
	\bs{z}_d}q(\theta_d,\bs{z}_d|\gamma,\phi)\log q(\theta_d,\bs{z}_d|\gamma,\phi)d\theta_d\\
&&=\E_{q}[p(\theta,\bs{z}_d,\bs{w}_d|\alpha,\beta)]-\E_{q}[q(\theta_d,\bs{z}_d|\gamma,\phi)]\\
&&=\E_{q}[p(\theta,\bs{z}_d,\bs{w}_d|\alpha,\beta)]+H(\gamma,\phi)\\
\ea
\ee
where the entropy-$H(\gamma,\phi)=-\E_{q}[q(\theta_d,\bs{z}_d|\gamma,\phi)]$.\\
\\
Right-hand side of the inequality\ref{elbo1} above is a lower bound on the log likelihood for an arbitrary variational distribution-$q(\theta,\bs{z}|\gamma,\phi)$.\\
\\
This lower bound can be denoted as $\mathcal{L}(\gamma,\phi,\alpha,\beta)$,\\
\be\label{elbo2}
\ba{rcl}
\mathcal{L}(\gamma,\phi,\alpha,\beta)=\Sum_{d=1}^{D}\left(\E_{q}[\log p(\theta_d|\alpha)]+\E_{q}[\log p(\bs{z}_d|\theta_d)]+\E_{q}[\log p(\bs{w}_d|\bs{z}_d,\beta)]-\E_q[\log q(\theta_d)]\right)
\ea
\ee
\section{Particle approximation and Evidence Lower Bound}
Motivated by the particle approximation in Rockova V. 2016, for $d=1,2,\ldots,D$, we use a weighted mixture of atoms to approximate $\pi(\bs{z}_{dn}|\mathcal{D}),n=1,2,\ldots,N_d,d=1,2,\ldots,D$, and we denote:\\
\[
q_{PEM}(\bs{z}_{dn}|\bs{\Gamma}_{dn},\bs{\omega})=\Sum_{p=1}^{P}\omega_{p}\mathbb{I}\{\bs{z}_{dn}=\bs{z}_{pdn}\}
\]
where $\bs{\Gamma}_{dn}=[\bs{z}_{1dn},\bs{z}_{2dn},\ldots,\bs{z}_{Pdn}]$,corresponding importance weights-$\bs{\omega}=(\omega_{1},\omega_{2},\ldots,\omega_{P})^T$, where $\sum_{p=1}^{P}\omega_{p}=1,\forall d=1,\ldots,D,n=1,2,\ldots,N_d;0\le\omega_{p}\le 1,\forall p=1,2,\ldots,P$.\\
For $\bs{z}_{pdn},\forall p=1,2,\ldots,P,n=1,2,\ldots,N_d,d=1,2,\ldots,D$, $z_{pdn}$ can only takes one of $1,2,\ldots,k$ values, representing the possible $k$ topics in LDA model. \\
\\
And denote $\bs{z}_{pdn}^{i}$ will only can take the $i=1,2,\ldots,k$ values corresponding to $k$ possible topics.\\
\[
z_{pdn}^i=\begin{cases}
1\quad\mbox{if word $z_{pdn}$ is from topic $i$}\\
0\quad otherwise
\end{cases}
\]
We follow the similar setup for the variational distribution setup for $q(\theta|\gamma)$ in D. Blei et al. 2003.\\
Thus the new variational distribution of $q_{PEM}(\theta_d,\bs{z}_d|\bs{\gamma}_d,\bs{\Gamma}_d,\bs{\omega})$ is:\\
\[
q_{PEM}(\theta,\bs{z}|\bs{\gamma},\bs{\Gamma},\bs{\omega})=\Prod_{d=1}^{D}\left[q(\theta_d|\bs{\gamma}_d)\Prod_{n_d=1}^{N_d}q_{PEM}(\bs{z}_{dn_d}|\bs{\Gamma}_d,\bs{\omega})\right]
\]
where $\bs{\gamma}=[\bs{\gamma}_1,\bs{\gamma}_2,\ldots,\bs{\gamma}_D],\bs{\omega}=[\omega_1,\omega_2,\ldots,\omega_P]$.\\
\\
Then the evidence lower bound-\ref{elbo2} replacing $q(\theta,\bs{z}|\gamma,\phi)$ with $q_{PEM}(\theta_d,\bs{z}_d|\bs{\gamma}_d,\bs{\Gamma}_d,\bs{\omega})$ can be written as :\\
\be\label{elbo3}
\ba{rcl}
\mathcal{L}_{\lambda}(\bs{\gamma},\bs{\Gamma},\bs{\omega};\bs{\theta},\alpha,\beta)&=&\Sum_{d=1}^{D}\left(\E_{q_{PEM}}[\log p(\theta_d|\alpha)]+\E_{q_{PEM}}[\log p(\bs{z}_d|\theta_d)]+\E_{q_{PEM}}[\log p(\bs{w}_d|\bs{z}_d,\beta)]\right.\\
&&\left.-\E_{q}[\log q(\theta_d|\bs{\gamma}_d)]-\lambda\E_{q_{PEM}}[\log q_{PEM}(\bs{z}_{n_d}|\bs{\Gamma}_d,\bs{\omega}_d)]\right)\\
&=&\Sum_{d=1}^{D}\left(\E_{q_{PEM}}[\log p(\theta_d|\alpha)]+\E_{q_{PEM}}[\log p(\bs{z}_d|\theta_d)]+\E_{q_{PEM}}[\log p(\bs{w}_d|\bs{z}_d,\beta)]\right.\\
&&+\Sum_{d=1}^{D}\left(-\E_{q}[\log q(\theta_d|\bs{\gamma}_d)]+H_\lambda(\bs{\Gamma}_d,\bs{\omega})\right)
\ea
\ee
where $H_\lambda(\bs{\Gamma}_d,\bs{\omega})=-\lambda\E_{q_{PEM}}[\log q_{PEM}(\bs{z}_d|\bs{\Gamma}_d,\bs{\omega})],
\lambda\ge 0$.\\
When $\lambda=1$, the above is regular ELBO from variational calculus. if $\lambda=0$, it is equivalent to parallel EM.\\
\\
In the E step of particle EM algorithm, we need to maximize the variational parameters-$\bs{\gamma},\bs{\Gamma},\bs{\omega}$,
\\
Finding $\hat{\bs{\Gamma}}_d=[\hat{\bs{Z}}_{1d},\hat{\bs{Z}}_{2d},\ldots,\hat{\bs{Z}}_{Pd}]$ and $\hat{\bs{\omega}}$ such that\\
\[
(\hat{\bs{\Gamma}},\hat{\bs{\omega}},\hat{\bs{\gamma}})=\underset{\bs{\Gamma},\bs{\omega},\bs{\gamma}}{\operatorname{argmax}}\mathcal{L}_{\lambda}(\bs{\gamma},\bs{\Gamma},\bs{\omega};\alpha,\beta) \quad\mbox{subject to}\Sum_{p=1}^P\omega_{p}=1,0\le\omega_{p}\le 1 
\]
where $\hat{\bs{\Gamma}}=[\hat{\bs{\Gamma}}_1,\hat{\bs{\Gamma}}_2,\ldots,\hat{\bs{\Gamma}}_D], \hat{\bs{\omega}}=[\hat{\bs{\omega}}_1,\hat{\bs{\omega}}_2,\ldots,\hat{\bs{\omega}}_D],\hat{\bs{\gamma}}=[\hat{\bs{\gamma}}_1,\hat{\bs{\gamma}}_2,\ldots,\hat{\bs{\gamma}}_D]$.\\
\\
Denote the $P^*_{dni}$ unique particles contained with each $\bs{\Gamma}_{dni}$ by: $\bs{\Gamma}_{ndi}^*=[\phi_{1dni}^*,\phi_{2dni}^*,\ldots,\phi_{P^*_{dni}dni}^*]$,denote $p_{l_{dni}}^*$ the cumulative importance weight associated with each unique particle-$\phi_{ldni}^*$,i.e.,\\
\[
p_{ldni}^*=\Sum_{p=1}^{P}\omega_{p}\mathbb{I}(\phi_{pdni}=\phi_{ldni}^*)
\]
Then the term related with $\bs{\Gamma},\bs{\omega}$ in entropy of ELBO--\ref{elbo3} can be expressed as:\\
\[
H_\lambda(\bs{\Gamma},\bs{\omega})=-\lambda\Sum_{d=1}^{D}\Sum_{n=1}^{N_d}\Sum_{i=1}^{k}\Sum_{l=1}^{P^*_{dni}}p_{ldni}^*\log(p_{ldni}^*)
\]
\section{Particle EM}
\subsection{E step with single particle}
Before entirely introduce the Particle EM algorithm in LDA model, firstly we assume single particle-$P=1$ and fix some document-$\bs{w}_d$, the E step at $m$ iteration, given the $\bs{Z}_d^{(m)}$ such that complete data surrogate objective function related with $\bs{Z}_d$:\\
\be\label{elbo4}
\ba{rcl}
Q(\bs{Z}_d|\bs{Z}_d^{(m)},\bs{w}_d)&=&\E_{\alpha,\beta,\theta_d|\bs{w}_d,\bs{Z}_d^{(m)}}\log \pi(\alpha,\beta,\theta_d|\bs{w}_d)\\
&=&\Sum_{n=1}^{N_d}\Sum_{i=1}^{k}\phi_{dni}(\Psi(\gamma_i^{(m)})-\Psi(\sum_{j=1}^{k}\gamma_j^{(m)}))\\
&&+\Sum_{n=1}^{N_d}\Sum_{i=1}^{k}\Sum_{j=1}^{V_d}\bs{z}_{dn}^{i(m)}w_n^j\log\beta_{ij}
\ea
\ee
where $\phi_{dni}=\E(z_{dn}^{i})=P(z_{dn}^{i}=1)$.\\
\\
Then focusing on the $z_{dn}$, then
\be\label{eqz1}
z_{dn}^{(m+1)}=\underset{z_{dn}\in\{1,2,\ldots,k\}}{\operatorname{argmax}}\left\{\Sum_{i=1}^{k}z_{dn}^{i}(\Psi(\gamma_i^{(m)})-\Psi(\sum_{j=1}^{k}\gamma_j^{(m)}))+\Sum_{i=1}^{k}\bs{z}_{dn}^{i(m)}\log\beta_{iv_d}
	\right\}
\ee
The equation-\ref{eqz1} can be treated as the log-likelihood function of a Categorical distribution with the probability-$\bs{\phi}_{nd}=[\phi_{dn1},\phi_{dn2},\ldots,\phi_{dnk}],\sum_{i=1}^{k}\phi_{dni}=1$ where $\phi_{dni}=\E(z_{dn}^{i}|\bs{Z}_d^{(m)},\bs{\gamma}_d^{(m)})=P(z_{dn}^{i}=1|\bs{Z}_d^{(m)},\bs{\gamma}_d^{(m)}),\forall i=1,2,\ldots,k$.\\
\\

The next update of $z_{dn}^{(m+1)}=i$ is obtained by  choosing the $i,1\le i\le k$ such that $\phi_{ndi}=\max\{\phi_{dn1},\phi_{dn2},\ldots,\phi_{dnk}\}$.


And under $P=1$ the entropy term:\\
\be\label{en1}
\ba{rcl}
-\E_{q}[\log q(\theta_d|\bs{\gamma}_d)]+H_\lambda(\bs{\Gamma}_d,\bs{\omega}_d)
&=&-\Psi(\sum_{j=1}^{k}\gamma_j)+\Sum_{i=1}^{k}\log\Gamma(\gamma_i)-\Sum_{i=1}^{k}(\gamma_i-1)(\Psi(\gamma_i)-\Psi(\sum_{j=1}^{k}\gamma_j))\\
&&+H_\lambda(\bs{\Gamma}_d,\bs{\omega}_d)
\ea
\ee
Then with single particle $P=1$ and fix some $d,\bs{w}_d$  the evidence lower bound-\ref{elbo3} becomes:
\be\label{elbo5}
\ba{rcl}
\mathcal{L}_{d\lambda}(\bs{\gamma}_d,\bs{\Gamma},\bs{\omega};\bs{\theta},\alpha,\beta)
&=&\log\Gamma(\sum_{j=1}^{k}\alpha_j)+\Sum_{i=1}^{k}(\alpha_i-1)(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))\\
&&+\Sum_{n=1}^{N_d}\Sum_{i=1}^{k}\phi_{dni}(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))\\
&&+\Sum_{n=1}^{N_d}\Sum_{i=1}^{k}\Sum_{j=1}^{V}\phi_{dni}w_n^j\log\beta_{ij}\\
&&-\Psi(\sum_{j=1}^{k}\gamma_{dj})+\Sum_{i=1}^{k}\log\Gamma(\gamma_{di})-\Sum_{i=1}^{k}(\gamma_{di}-1)(\Psi(\gamma_i)-\Psi(\sum_{j=1}^{k}\gamma_{dj}))\\
&&+H_\lambda(\bs{\Gamma}_d,\bs{\omega}_d)\\
\ea
\ee
Firstly maximize the ELBO-\ref{elbo5} with respect to $\phi_{dni}$ with the constraint-$\sum_{j=1}^{k}\phi_{dnj}=1$.
Denote $\beta_{iv_d}=p(w_n^{v_d}=1|z_d^i=1)$ for the appropriate $v_d$.
Then add the Lagrange multiplier to the terms in ELBO-\ref{elbo5} containing $\phi_{ni}$,\\
\[
\mathcal{L}_{d[\phi_{dni}]}=\phi_{dni}(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))+\phi_{dni}\log\beta_{iv_d}-\lambda\phi_{dni}\log\phi_{dni}+\lambda_{\phi}(\sum_{j=1}^{k}\phi_{dnj}-1)
\]
Take first derivative in terms of $\phi_{dni}$, then:\\
\[
\frac{\partial\mathcal{L}_d}{\partial \phi_{dni}}=\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj})+\log\beta_{iv_d}-\lambda-\lambda\log\phi_{dni}+\lambda_{\phi}
\]
Set the derivative above equal to zero, then the value of estimated $\hat{\phi}_{dni}$ maximizing the ELBO-\ref{elbo5} is:\\
\[
\hat{\phi}_{dni}=\frac{\beta_{iv_d}^{1/\lambda}\exp((\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))/\lambda)}{\sum_{i=1}^{k}\beta_{iv_d}^{1/\lambda}\exp((\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))/\lambda)}
\]
Next, we maximize the equation-\ref{elbo5} with respect to $\gamma_{di}$, the terms containing $\gamma_i$ are:\\
\be\label{elbo6}
\ba{rcl}
\mathcal{L}_{d[\gamma_{di}]}&=&(\alpha_i-1)\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj})\Sum_{j=1}^{k}(\alpha_j-1)+\Sum_{n=1}^{N_d}\phi_{dni}(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))\\
&&-\Psi(\sum_{j=1}^{k}\gamma_j)+\Sum_{i=1}^{k}\log\Gamma(\gamma_{di})-(\gamma_{di}-1)\Psi(\gamma_{di})+\Psi(\sum_{j=1}^{k}\gamma_{dj})\Sum_{j=1}^{k}(\gamma_{dj}-1)
\ea
\ee
Take first derivative in terms of $\gamma_{di}$:
\[
\frac{\partial \mathcal{L}_{d[\gamma_{di}]}}{\partial \gamma_{di} }=(\alpha_i+\sum_{n=1}^{N_d}\phi_{dni}-\gamma_{di})\Psi'(\gamma_{di})-\Psi'(\sum_{i=1}^{k}\gamma_{di})\Sum_{j=1}^{k}(\alpha_j+\sum_{n=1}^{N_d}\phi_{dni}-\gamma_{dj})
\]
Setting the above first derivative equal to 0 then the value of estimated $\hat{\gamma}_{di}$ maximizing the ELBO-\ref{elbo5} is:\\
\[
\hat{\gamma}_{di}=\alpha_i+\sum_{n=1}^{N_d}\phi_{dni}
\]
Another way to represent the updated $Z_d$ given $Z_d^{(m)}$, then with single particle $P=1$ and fix some $d,\bs{w}_d$  the evidence lower bound-\ref{elbo3} becomes:
\be\label{elbo7}
\ba{rcl}
Q(\bs{Z}_d,\bs{\gamma}_d;\alpha,\beta,\theta_d|Z_d^{(m)})&=&\E_{\alpha,\beta,\theta_d|\bs{w}_d}\log \pi(\alpha,\beta,\theta_d|\bs{w}_d,Z_d^{(m)})\\
&=&\log\Gamma(\sum_{j=1}^{k}\alpha_j)-\Sum_{i=1}^{k}(\alpha_i-1)(\Psi(\gamma_{di})-\Psi(\sum_{i=1}^{k}\gamma_{di})\\
&&+\Sum_{n=1}^{N_d}\Sum_{i=1}^{k}z_{dn}^{i(m)}(\Psi(\gamma_i)-\Psi(\sum_{i=1}^{k}\gamma_i)\\
&&+\Sum_{n=1}^{N_d}\Sum_{i=1}^{k}\Sum_{j=1}^{V_d}z_{dn}^{i(m)}w_n^j\log\beta_{ij}\\
&&-\Psi(\sum_{j=1}^{k}\gamma_j)+\Sum_{i=1}^{k}\log\Gamma(\gamma_{di})-\Sum_{i=1}^{k}(\gamma_{di}-1)(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_j))\\
&&-\lambda\Sum_{n=1}^{N_d}\Sum_{i=1}^{k}z_{dn}^{i(m)}\log\phi_{dni}\\
\ea
\ee
where $\phi_{dni}=p(Z_{dn}^{i}=1),i=1,2,\ldots,k$.
With similar method as above, we can get the estimate he value of estimated $\hat{\phi}_{dni}$ maximizing the ELBO-\ref{elbo7} is:\\
\subsection{M step with single particle}
In M step with single particle-$P=1$, with all the documents-$\bs{w}_d,d=1,\ldots,D$, maximize the  ELBO-\ref{elbo5} with respect to model parameters-$\alpha,\beta$.\\
\\
Similarly to Blei et al. 2003, choose the terms related to $\beta$ and add Lagrange multiplier,\\
\[
\mathcal{L}_{[\beta]}=\Sum_{d=1}^{D}\Sum_{n=1}^{N_d}\Sum_{i=1}^{k}\Sum_{j=1}^{V}\phi_{dni}w_{dn}^j\log\beta_{ij}+\Sum_{i=1}^{k}\lambda_{\beta i}(\Sum_{j=1}^{V}\beta_{ij}-1)
\]
Take the first derivative in terms of $\beta_{ij}$, set it to zero, then:\\
\[
\hat{\beta}_{ij}=\frac{\sum_{d=1}^{D}\sum_{n=1}^{N_d}\phi_{dni}w_{dn}^j}{\sum_{d=1}^{D}\sum_{n=1}^{N_d}\sum_{j'=1}^{V}\phi_{dni}w_{dn}^{j'}}
\]
And also choose the terms containing $\alpha$:\\
\[
\mathcal{L}_{[\alpha]}=\Sum_{d=1}^{D}\left(\log\Gamma(\sum_{j=1}^{k}\alpha_j)+\Sum_{i=1}^{k}(\alpha_i-1)(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))\right)
\]
Take the first derivative in terms of $\alpha_{i}$,  then:\\
\[
\frac{\partial \mathcal{L}}{\partial \alpha_{i} }=D(\Psi(\sum_{j=1}^{k}\alpha_j)-\Psi(\alpha_i))+\Sum_{d=1}^{D}(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))
\]
Set it to zero, then $\forall i\neq j=1,2,\ldots,k$,:\\
\[
\frac{\partial \mathcal{L}}{\partial \alpha_{i}\alpha_{j} }=\delta(i,j)D\Psi'(\alpha_i)-\Psi'(\sum_{j=1}^{k}\alpha_j)
\]
\subsection{E step with multiple particles}
With $P>1$, we need alternatively collaborative updating the particle location-$[\bs{\phi}_{1dn},\bs{\phi}_{2dn},\ldots,\bs{\phi}_{Pdn}],\forall n=1,2,\ldots,N_d$ and their corresponding importance weights-$(\bs{\omega}_{1},\bs{\omega}_{2},\ldots,\bs{\omega}_{P})^T,\forall d=1,\ldots,D;n=1,2,\ldots,N_d$.\\
\\
Denote $\bs{\Gamma}^{(m)}=[\bs{\Gamma}_{1dn}^{(m)},\bs{\Gamma}_{2dn}^{(m)},\ldots,\bs{\Gamma}_{Ddn}^{(m)}],\forall d=1,\ldots,D;n=1,2,\ldots,N_d$, the state of  particle system at the $m$th iteration and denote $\bs{\omega}^{(m)}=(\bs{\omega}_{1dn}^{(m)},\bs{\omega}_{2dn}^{(m)},\ldots,\bs{\omega}_{Ddn}^{(m)})^T$.\\
\\
Given $\bs{\omega}^{(m)}$ fix some $d$ then the evidence lower bound-\ref{elbo3} becomes:\\
\be\label{elbo7}
\ba{rcl}
\mathcal{L}_{d\lambda}(\bs{\gamma}_d,\bs{\Gamma},\bs{\omega};\bs{\theta},\alpha,\beta)
&=&\log\Gamma(\sum_{j=1}^{k}\alpha_j)+\Sum_{i=1}^{k}(\alpha_i-1)(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))\\
&&+\Sum_{p=1}^{P}\Sum_{n=1}^{N_d}\omega_{p}^{(m)}\Sum_{i=1}^{k}\phi_{pdni}(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))\\
&&+\Sum_{p=1}^{P}\Sum_{n=1}^{N_d}\omega_{p}^{(m)}\Sum_{i=1}^{k}\Sum_{j=1}^{V}\phi_{pdni}w_n^j\log\beta_{ij}\\
&&-\Psi(\sum_{j=1}^{k}\gamma_{dj})+\Sum_{i=1}^{k}\log\Gamma(\gamma_{di})-\Sum_{i=1}^{k}(\gamma_{di}-1)(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))\\
&&-\lambda\Sum_{p=1}^{P}\Sum_{n=1}^{N_d}\omega_{p}\Sum_{i=1}^{k}\phi_{pdni}\log(\sum_{p=1}^{P}\omega_{p}\phi_{pdni})
\ea
\ee
where $\phi_{pdni}=\E(z_{pdn}^{i})=P(z_{pdn}^{i}=1),\forall p=1,2,\ldots,P;d=1,2,\ldots,D,n=1,2,\ldots,N_d,i=1,2,\ldots,k$.\\
Similar to one particle system,firstly maximize the ELBO-\ref{elbo7} with respect to $\phi_{pdni}$ with the constraint-$\sum_{j=1}^{k}\phi_{pdnj}=1$.
Denote $\beta_{iv_d}=p(w_n^{v_d}=1|z_{pd}^i=1)$ for the appropriate $v_d$.\\
Then  add the Lagrange multiplier to the terms in ELBO-\ref{elbo7} containing $\phi_{pdni}$,\\
\be\label{phi1}
\ba{rcl}
\mathcal{L}_{d[\phi_{pdni}]}&=&\omega_{p}\phi_{pdni}(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))+\omega_{p}\phi_{pdni}\log\beta_{iv_d}\\&&-\lambda\Sum_{p'=1}^{P}\omega_{p'}\phi_{p'dni}\log(\sum_{p'=1}^{P}\omega_{p'}\phi_{p'dni})
+\lambda_{\phi_p}(\sum_{j=1}^{k}\phi_{pdnj}-1)
\ea
\ee
Take first derivative in terms of $\phi_{pdni}$, then:\\
\[
\frac{\partial\mathcal{L}_d}{\partial \phi_{pdni}}=\omega_{p}(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))+\omega_{p}\log\beta_{iv_d}-\lambda\omega_{p}(\log(\sum_{p'=1}^{P}\omega_{p'}\phi_{p'dni})+1)+\lambda_{\phi_p}
\]
Take second derivative in terms of $\phi_{pdni}$, then:\\
\[
\frac{\partial^2\mathcal{L}_d}{\partial \phi^2_{pdni}}=-\frac{\lambda\omega_p^2}{\sum_{p'=1}^{P}\omega_{p'}\phi_{p'dni}}
\]
And the second order partial derivative  in terms of $\phi_{pdni},\phi_{p''dni}$ where $\forall p\neq p''=1,2,\ldots,P$,\\
\[
\frac{\partial^2\mathcal{L}_d}{\partial \phi_{pdni}\partial \phi_{p''dni}}=-\frac{\lambda\omega_p\omega_{p''}}{\sum_{p'=1}^{P}\omega_{p'}\phi_{p'dni}}
\]
Apply the Newton-Raphson method, for each fixed $n=1,2,\ldots,N_d,d=1,2,\ldots,D$, denote $\bs{\phi}_{dni}=[\phi_{1dni},\phi_{2dni},\ldots,\phi_{Pdni}]^T$, then by iterating the equation below we can find the maximal-$\bs{\phi}_{dn}$:\\
\[
\bs{\phi}_{dni(new)}=\bs{\phi}_{dni(old)}-H(\bs{\phi}_{dn(old)})^{-1}g(\bs{\phi}_{dni(old)})
\]
where $H(\bs{\phi}_{dn}),g(\bs{\phi}_{dni})$ are the Hessian matrix and gradient respectively at the point-$\bs{\phi}_{dni}$ defined above.\\
\\
A point need to note that the maximal $\bs{\phi}_{dni}$ achieve above need to satisfy $\Sum_{i=1}^{k}\phi_{pdni}=1$ for each fixed-$n=1,2,\ldots,N_d,d=1,2,\ldots,D,p=1,2,\ldots,P$.\\
\\
In order to address the constraints-$\forall p=1,2,\ldots,P,n=1,2,\ldots,N_d,d=1,2,\ldots,D,i=1,2,\ldots,k, 0\le\phi_{pdni}\le 1$, and for each fixed-$p,n,d,\Sum_{i=1}^{k}\phi_{pdni}=1$, we transform $\phi_{pdni}$ to some variable-$x_{pdni}\in(-\infty,\infty)$ with $x_{pdni}=logit(\phi_{pdni}),\phi_{pndi}=\frac{e^{x_{pdni}}}{1+e^{x_{pdni}}},\forall p
=1,2,\ldots,P,n=1,2,\ldots,N_d,d=1,2,\ldots,D,i=1,2,\ldots,k-1$ and based on the constraints-$\Sum_{i=1}^{k}\phi_{pdni}=1$, we have total $P(k-1)\sum_{d=1}^{D}N_d$ free parameters with $\phi_{pndk}=1-\Sum_{i'=1}^{k-1}\frac{e^{x_{pdni'}}}{1+e^{x_{pdni'}}}$.\\
\\
Apply the chain rule of differentiation, then the first derivative in terms of the new parameter-$x_{pdni'},\forall p=1,2,\ldots,P,d=1,2,\ldots,D,n=1,2,\ldots,N_d,i'=1,2,\ldots,k-1$,\\
\be
\ba{rcl}
\frac{\partial\mathcal{L}_d}{\partial x_{pdni'}}=\frac{\partial\mathcal{L}_d}{\partial \phi_{pdni'}}\frac{\partial\phi_{pdni'}}{\partial x_{pdni'}}&=&\left(\omega_{p}(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))+\omega_{p}\log\beta_{iv_d}-\lambda\omega_{p}(\log(\sum_{p'=1}^{P}\omega_{p'}\frac{e^{x_{pdni'}}}{1+e^{x_{pdni'}}})+1)\right)\\
&&\times\frac{e^{x_{pdni'}}}{(1+e^{x_{pdni'}})^2}
\ea
\ee
 Also the second derivative in terms of the parameter-$x_{pdni'},\forall p=1,2,\ldots,P,d=1,2,\ldots,D,n=1,2,\ldots,N_d,i'=1,2,\ldots,k-1$,\\
 \be
 \ba{rcl}
 \frac{\partial^2\mathcal{L}_d}{\partial x_{pdni'}^2}&=&\frac{\partial}{\partial x_{pdni'}}\left(\frac{\partial\mathcal{L}_d}{\partial x_{pdni'}}\right)=\frac{\partial}{\partial x_{pdni'}}\left(\frac{\partial\mathcal{L}_d}{\partial \phi_{pdni'}}\frac{\partial\phi_{pdni'}}{\partial x_{pdni'}}\right)\\
 &=&\frac{\partial\mathcal{L}_d}{\partial \phi_{pdni'}}\frac{\partial^2\phi_{pdni'}}{\partial x_{pdni'}^2}+\frac{\partial^2\mathcal{L}_d}{\partial \phi_{pdni'}\partial x_{pdni'}}\frac{\partial\phi_{pdni'}}{\partial x_{pdni'}}\\
 &=&\frac{\partial\mathcal{L}_d}{\partial \phi_{pdni'}}\frac{\partial^2\phi_{pdni'}}{\partial x_{pdni'}^2}+\frac{\partial^2\mathcal{L}_d}{\partial \phi_{pdni'}^2}(\frac{\partial\phi_{pdni'}}{\partial x_{pdni'}})^2\\
 &=&\frac{\partial\mathcal{L}_d}{\partial \phi_{pdni'}}\frac{\partial\phi_{pdni'}}{\partial x_{pdni'}}\\
 &=&\left(\omega_{p}(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))+\omega_{p}\log\beta_{iv_d}-\lambda\omega_{p}(\log(\sum_{p'=1}^{P}\omega_{p'}\frac{e^{x_{p'dni'}}}{1+e^{x_{p'dni'}}})+1)\right)\\
 &&\times\frac{e^{x_{pdni'}}}{(1+e^{x_{pdni'}})^2}(1-\frac{e^{x_{pdni'}}}{1+e^{x_{pdni'}}})-\frac{\lambda\omega_p^2}{\sum_{p'=1}^{P}\omega_{p'}\frac{e^{x_{pdni'}}}{1+e^{x_{pdni'}}}}*\frac{e^{2x_{pdni'}}}{(1+e^{x_{pdni'}})^4}\\
 \ea
 \ee
Also the second order partial derivative in terms of the parameter-$x_{pdni'},x_{p''dni'},\forall p\neq p''=1,2,\ldots,P,d=1,2,\ldots,D,n=1,2,\ldots,N_d,i'=1,2,\ldots,k-1$,\\
\be
\ba{rcl}
\frac{\partial^2\mathcal{L}_d}{\partial x_{pdni'}\partial x_{p''dni'}}&=&\frac{\partial}{\partial x_{p''dni'}}\left(\frac{\partial\mathcal{L}_d}{\partial x_{pdni'}}\right)=\frac{\partial}{\partial x_{p''dni'}}\left(\frac{\partial\mathcal{L}_d}{\partial \phi_{pdni'}}\frac{\partial\phi_{pdni'}}{\partial x_{pdni'}}\right)\\
&=&\frac{\partial\mathcal{L}_d}{\partial \phi_{pdni'}}\frac{\partial^2\phi_{pdni'}}{\partial x_{pdni'}\partial x_{p''dni'}}+\frac{\partial^2\mathcal{L}_d}{\partial \phi_{pdni'}\partial x_{p''dni'}}\frac{\partial\phi_{pdni'}}{\partial x_{pdni'}}\\
&=&\frac{\partial\mathcal{L}_d}{\partial \phi_{pdni'}}\times 0+\frac{\partial^2\mathcal{L}_d}{\partial \phi_{pdni'}\partial \phi_{p''dni'}}\frac{\partial\phi_{pdni'}}{\partial x_{pdni'}}\frac{\partial\phi_{p''dni'}}{\partial x_{p''dni'}}\\
&=&\frac{\partial^2\mathcal{L}_d}{\partial \phi_{pdni'}\partial \phi_{p''dni'}}\frac{\partial\phi_{pdni'}}{\partial x_{pdni'}}\frac{\partial\phi_{p''dni'}}{\partial x_{p''dni'}}\\
&=&-\frac{\lambda\omega_p\omega_{p''}}{\sum_{p'=1}^{P}\omega_{p'}\frac{e^{x_{p'dni'}}}{1+e^{x_{p'dni'}}}}\frac{e^{x_{pdni'}}}{(1+e^{x_{pdni'}})^2}\frac{e^{x_{p''dni'}}}{(1+e^{x_{p''dni'}})^2}\\
\ea
\ee
Thus the Newton-Raphson method in terms of the vector of $P$ parameters-$\bs{x}_{dni'}=[x_{1dni'}, x_{2dni'}, \ldots, x_{Pdni'}]^T,\forall i'=1,2,\ldots,k-1,n=1,2,\ldots,N_d,d=1,2,\ldots,D$ without constraints, thus  by iterating the equation below we can find the maximal-$\bs{x}_{dni'}$:\\
\[
\bs{x}_{dni'(new)}=\bs{x}_{dni'(old)}-H(\bs{x}_{dni'(old)})^{-1}g(\bs{x}_{dni'(old)})
\]
where $H(\bs{x}_{dni'}),g(\bs{x}_{dni'})$ are the Hessian matrix and gradient respectively at the vector point-$\bs{x}_{dni'}$ defined above respectively.\\
\\
Apply the property of importance weights for any fixed $\Sum_{p'=1}^{P}\omega_{p'}=1$,
and add the $P$ equations-$\sum_{p'=1}^{P}\frac{\partial\mathcal{L}_d}{\partial \phi_{p'dni}}$ and set it equal to zero, then:\\
\[
\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj})+\log\beta_{iv_d}-\lambda(\log(\sum_{p'=1}^{P}\omega_{p'
	}\phi_{p'dni})+1)+\sum_{p'=1}^{P}\lambda_{\phi_{p'}}=0
\]
\[
\log(\sum_{p'=1}^{P}\omega_{p'dn}\phi_{p'dni})=(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))/\lambda+\log\beta_{iv_d}/\lambda+\sum_{p'=1}^{P}\lambda_{\phi_{p'}}/\lambda-1
\]
\\
Secondly, we need to update the particle importance weights-$\bs{\omega}=[\omega_1,\omega_2,\ldots,\omega_P]^T$ given $\hat{\phi}_{pdni},\forall i=1,2,\ldots,k,n=1,2,\ldots,N_d,d=1,2,\ldots,D$,
Then the ELBO-\ref{elbo7} can be written as:\\
\be\label{elbo8}
\ba{rcl}
\mathcal{L}_{\lambda}(\bs{\gamma},\bs{\Gamma},\bs{\omega};\bs{\theta},\alpha,\beta)
&=&D\log\Gamma(\sum_{j=1}^{k}\alpha_j)+\Sum_{d=1}^{D}\Sum_{i=1}^{k}(\alpha_i-1)(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))\\
&&+\Sum_{d=1}^{D}\Sum_{n=1}^{N_d}\Sum_{i=1}^{k}\Sum_{l=1}^{P}\omega_{p}\phi_{pdni}(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))\\
&&+\Sum_{d=1}^{D}\Sum_{n=1}^{N_d}\Sum_{i=1}^{k}\Sum_{j=1}^{V}\Sum_{p=1}^{P}\omega_{p}\phi_{pdni}w_n^j\log\beta_{ij}\\
&&-\Sum_{d=1}^{D}\left(\Psi(\sum_{j=1}^{k}\gamma_{dj})+\Sum_{i=1}^{k}\log\Gamma(\gamma_{di})-\Sum_{i=1}^{k}(\gamma_{di}-1)(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))\right)\\
&&-\lambda\Sum_{d=1}^{D}\Sum_{n=1}^{N_d}\Sum_{i=1}^{k}\Sum_{p=1}^{P}\omega_p\phi_{pdni}\log(\Sum_{p=1}^{P}\omega_p\phi_{pdni})
\ea
\ee
Maximize the ELBO-\ref{elbo8} with respect to 
$\omega_{p}$ with the constraint-$\sum_{p'=1}^{P}\omega_{p'}=1$.
Denote $\beta_{iv_d}=p(w_n^{v_d}=1|z_{pd}^i=1)$ for the appropriate $v_d$.\\
Then  add the Lagrange multiplier to the terms in ELBO-\ref{elbo8} containing $\omega_{p},\forall p=1,2,\ldots,P$,\\
\be\label{omg1}
\ba{rcl}
\mathcal{L}_{[\omega_{p}]}&=&\Sum_{d=1}^{D}\Sum_{n=1}^{N_d}\Sum_{i=1}^{k}\omega_{p}\phi_{pdni}(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))\\
&&+\Sum_{d=1}^{D}\Sum_{n=1}^{N_d}\Sum_{i=1}^{k}\omega_{p}\phi_{pdni}\log\beta_{iv_d}\\
&&-\lambda\Sum_{d=1}^{D}\Sum_{n=1}^{N_d}\Sum_{i=1}^{k}\Sum_{p'=1}^{P}\omega_{p'}\phi_{p'dni}\log(\Sum_{p'=1}^{P}\omega_{p'}\phi_{p'dni})
+\lambda_{\omega_{p'}}(\sum_{p'=1}^{P}\omega_{p'}-1)
\ea
\ee
Thus for any $p=1,2,\ldots,P$  given $\hat{\phi}_{pndi},\forall p=1,2,\ldots,P,i=1,2,\ldots,k,n=1,2,\ldots,N_d,d=1,2,\ldots,D$\\
\[
\hat{\omega}_p=\underset{\omega_p}{\operatorname{argmax}}\mathcal{L}_{[\omega_{p}]}
\]
where $0\le \hat{\omega}_p\le 1,\Sum_{p'=1}^{P}\omega_{p'}=1,\forall p=1,2,\ldots,P$.
Based on the expression of $\mathcal{L}_{[\omega_{p}]}$ above,\\
\be\label{omg3}
\ba{rcl}
\hat{\omega}_p&\propto&\Sum_{d=1}^{D}\Sum_{n=1}^{N_d}\Sum_{i=1}^{k}\left(\phi_{pdni}(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))+\phi_{pdni}\log\beta_{iv_d}\right)\\
&=&\frac{\sum_{d=1}^{D}\sum_{n=1}^{N_d}\sum_{i=1}^{k}\left(\phi_{pdni}(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))+\phi_{pdni}\log\beta_{iv_d}\right)}{\sum_{p'=1}^{P}\sum_{d=1}^{D}\sum_{n=1}^{N_d}\sum_{i=1}^{k}\left(\phi_{p'dni}(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))+\phi_{p'dni}\log\beta_{iv_d}\right)}
\ea
\ee

Take first derivative in terms of $\omega_{p}$, then:\\
\be\label{omg2}
\ba{rcl}
\frac{\partial\mathcal{L}_{[\omega_{p}]}}{\partial \omega_{p}}&=&\Sum_{d=1}^{D}\Sum_{n=1}^{N_d}\Sum_{i=1}^{k}(\phi_{pdni}(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))\\
&&+\Sum_{d=1}^{D}\Sum_{n=1}^{N_d}\Sum_{i=1}^{k}\phi_{pdni}\log\beta_{iv_d}\\
&&-\lambda\Sum_{d=1}^{D}\Sum_{n=1}^{N_d}\Sum_{i=1}^{k}\phi_{pdni}\log(\Sum_{p'=1}^{P}\omega_{p'}\phi_{p'dni})-\lambda\Sum_{d=1}^{D}\Sum_{n=1}^{N_d}\Sum_{i=1}^{k}\phi_{pdni}+\lambda_{\omega_{p'}}
\ea
\ee
Last, we maximize the equation-\ref{elbo7} with respect to $\gamma_{di}$, the terms containing $\gamma_{di}$ are:\\
\be\label{elbo9}
\ba{rcl}
\mathcal{L}_{d[\gamma_{di}]}&=&(\alpha_i-1)\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj})\Sum_{j=1}^{k}(\alpha_j-1)+\sum_{n=1}^{N_d}\sum_{p=1}^{P}\omega_{p}\phi_{pdni}(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))\\
&&-\Psi(\sum_{j=1}^{k}\gamma_j)+\Sum_{i=1}^{k}\log\Gamma(\gamma_{di})-(\gamma_{di}-1)\Psi(\gamma_{di})+\Psi(\sum_{j=1}^{k}\gamma_{dj})\Sum_{j=1}^{k}(\gamma_{dj}-1)
\ea
\ee
Take first derivative in terms of $\gamma_{di}$:
\[
\frac{\partial \mathcal{L}_{d[\gamma_{di}]}}{\partial \gamma_{di} }=(\alpha_i+\sum_{n=1}^{N_d}\sum_{p=1}^{P}\omega_{p}\phi_{pdni}-\gamma_{di})\Psi'(\gamma_{di})-\Psi'(\sum_{i=1}^{k}\gamma_{di})\Sum_{j=1}^{k}(\alpha_j+\sum_{n=1}^{N_d}\sum_{p=1}^{P}\omega_{p}\phi_{pdni}-\gamma_{dj})
\]
Setting the above first derivative equal to 0 then the value of estimated $\hat{\gamma}_{di}$ maximizing the ELBO-\ref{elbo5} is:\\
\[
\hat{\gamma}_{di}=\alpha_i+\sum_{n=1}^{N_d}\sum_{p=1}^{P}\omega_{p}\phi_{pdni}
\]
\subsection{M step with multiple particles}
In M step with multiple particles-$P>1$, with all the documents-$\bs{w}_d,d=1,\ldots,D$, maximize the  ELBO-\ref{elbo8} with respect to model parameters-$\bs{\alpha,}\bs{\beta}$.\\
\\
Similarly to Blei et al. 2003, choose the terms related to $\beta$ and add Lagrange multiplier,\\
\[
\mathcal{L}_{[\beta]}=\Sum_{d=1}^{D}\Sum_{n=1}^{N_d}\Sum_{i=1}^{k}\Sum_{j=1}^{V}\Sum_{p=1}^{P}\omega{p}\phi_{pdni}w_{dn}^j\log\beta_{ij}+\Sum_{i=1}^{k}\lambda_{\beta i}(\Sum_{j=1}^{V}\beta_{ij}-1)
\]
Take the first derivative in terms of $\beta_{ij}$, set it to zero, then:\\
\[
\hat{\beta}_{ij}=\frac{\sum_{d=1}^{D}\sum_{n=1}^{N_d}\sum_{p=1}^{P}\omega_{p}\phi_{pdni}w_{dn}^j}{\sum_{d=1}^{D}\sum_{n=1}^{N_d}\sum_{j'=1}^{V}\sum_{p=1}^{P}\omega_{p}\phi_{pdni}w_{dn}^{j'}}
\]
And also choose the terms containing $\alpha$:\\
\[
\mathcal{L}_{[\alpha]}=\Sum_{d=1}^{D}\left(\log\Gamma(\sum_{j=1}^{k}\alpha_j)+\Sum_{i=1}^{k}(\alpha_i-1)(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))\right)
\]
Take the first derivative in terms of $\alpha_{i},\forall i=1,2,\ldots,k$,  then:\\
\[
\frac{\partial \mathcal{L}}{\partial \alpha_{i} }=D(\Psi(\sum_{j=1}^{k}\alpha_j)-\Psi(\alpha_i))+\Sum_{d=1}^{D}(\Psi(\gamma_{di})-\Psi(\sum_{j=1}^{k}\gamma_{dj}))
\]
Set it to zero, then $\forall i\neq j=1,2,\ldots,p$,:\\
\[
\frac{\partial \mathcal{L}}{\partial \alpha_{i}\alpha_{j} }=\delta(i,j)D\Psi'(\alpha_i)-\Psi'(\sum_{j=1}^{k}\alpha_j)
\]
Apply the Newton-Raphson method for a Hessian with special structure, denote $\bs{\alpha}=[\alpha_{1},\alpha_{2},\ldots,\alpha_{k}]^T$, then by iterating the equation below we can find the maximal-$\bs{\alpha}$:\\
\[
\bs{\alpha}_{(new)}=\bs{\alpha}_{(old)}-H'(\bs{\alpha}_{(old)})^{-1}g'(\bs{\alpha}_{(old)})
\]
where $H'(\bs{\alpha}),g'(\bs{\alpha})$ are the Hessian matrix and gradient respectively at the point-$\bs{\alpha}$ defined above.\\
And the Hessian matrix $H'(\bs{\alpha})$ with the special form:\\
\[
H'(\bs{\alpha})=diag(\bs{h})-\Psi'(\sum_{j=1}^{k}\alpha_j)\bs{1}\bs{1}^T
\]
where $\bs{h}=[D\Psi'(\alpha_1),D\Psi'(\alpha_2),\ldots,D\Psi'(\alpha_k)]^T$\\
\\
And inverse of Hessian matrix $H'(\bs{\alpha})$ can be expressed as:\\
\[
H'(\bs{\alpha})^{-1}=diag(\bs{h})^{-1}+\frac{Ddiag(\bs{h})^{-1}\bs{1}\bs{1}^Tdiag(\bs{h})^{-1}}{D(\Psi'(\sum_{j=1}^{k}\alpha_j))^{-1}-\sum_{j=1}^{k}(\Psi'(\alpha_j))^{-1}}
\]

\end{document}